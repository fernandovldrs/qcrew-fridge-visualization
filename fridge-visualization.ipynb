{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox, Layout, Box\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing functions\n",
    "\n",
    "def log_parsing(log_folder, data_type):\n",
    "    '''\n",
    "    This function reads the log files stored in log_folder and returns a pandas dataframe.\n",
    "    data_type ('maxigauge', 'Status', 'Flowmeter', 'Temperature'): states which type of data\n",
    "    is to be read from the log files.\n",
    "    '''\n",
    "    \n",
    "    log_folder = log_folder.strip()\n",
    "    if log_folder[-1] != '/':\n",
    "        log_folder += '/'\n",
    "    \n",
    "    # Get list of dates registered in log_folder. \n",
    "    list_folders_log = os.listdir(log_folder)\n",
    "    list_date_log = []\n",
    "\n",
    "    # Filter the valid folders\n",
    "    for folder in list_folders_log:\n",
    "        try:\n",
    "            folder_timestamp = pd.to_datetime(folder, format = \"%y-%m-%d\")\n",
    "            list_date_log.append({'str': folder, \n",
    "                                  'ts': folder_timestamp})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Sort valid entries by date. This is important to later identify\n",
    "    # in which periods of time there's no entry.\n",
    "    list_date_log.sort(key = lambda x: x['ts'])\n",
    "    \n",
    "    if data_type == 'maxigauge':\n",
    "        \n",
    "        column_names = ['CH1', 'CH2', 'CH3', 'CH4', 'CH5', 'CH6']\n",
    "        date_col = []\n",
    "        CH_col = [[], [], [], [], [], []]\n",
    "        \n",
    "        is_first_log = True   \n",
    "        for date_log in list_date_log:\n",
    "            log_name = log_folder + date_log['str'] + '/maxigauge ' + date_log['str'] + '.log'\n",
    "            \n",
    "            # Check whether file exists\n",
    "            if not os.path.isfile(log_name):\n",
    "                continue\n",
    "                \n",
    "            with open(log_name, 'r') as log_file:\n",
    "                log_lines = log_file.readlines()\n",
    "                \n",
    "                # Collect a column of data for each registered instant\n",
    "                for line in log_lines:\n",
    "                    line = line.split(',')\n",
    "                    \n",
    "                    # Fill date column\n",
    "                    date = line[0].replace(' ','') + ' ' + line[1].replace(' ','')\n",
    "                    date_col.append(pd.to_datetime(date, format = \"%d-%m-%y %H:%M:%S\"))\n",
    "                    \n",
    "                    # Fill each channel's data column. Note that CH1 signal corresponds to CH_col[0]\n",
    "                    CH_col[0].append(float(line[5]))\n",
    "                    CH_col[1].append(float(line[11]))\n",
    "                    CH_col[2].append(float(line[17]))\n",
    "                    CH_col[3].append(float(line[23]))\n",
    "                    CH_col[4].append(float(line[29]))\n",
    "                    CH_col[5].append(float(line[35]))\n",
    "                    \n",
    "                    # Fill a line with NaN's if non-logged period is larger than 5 minutes\n",
    "                    if is_first_log:\n",
    "                        is_first_log = False\n",
    "                        continue\n",
    "\n",
    "                    if (pd.Timedelta(date_col[-1] - date_col[-2]).seconds)/60.0 > 5:\n",
    "                        non_logged = pd.date_range(start = date_col[-2], end = date_col[-1], periods = 3)[1]\n",
    "                        date_col.append(non_logged)\n",
    "                        CH_col[0].append(np.nan)\n",
    "                        CH_col[1].append(np.nan)\n",
    "                        CH_col[2].append(np.nan)\n",
    "                        CH_col[3].append(np.nan)\n",
    "                        CH_col[4].append(np.nan)\n",
    "                        CH_col[5].append(np.nan)\n",
    "                    \n",
    "        data_dict = {column_names[i]: CH_col[i] for i in range(len(column_names))}\n",
    "        index = pd.DatetimeIndex(date_col)\n",
    "        \n",
    "        return pd.DataFrame(data = data_dict, index = index).sort_index()\n",
    "    \n",
    "    if data_type == 'Status':\n",
    "        \n",
    "        column_names = ['nxdsf', 'nxdsct', 'nxdst', 'nxdsbs', 'nxdstrs', 'tc400errorcode', 'tc400ovtempelec', \n",
    "                        'tc400ovtemppump', 'tc400setspdatt', 'tc400pumpaccel', 'tc400commerr', 'ctrl_pres', \n",
    "                        'cpastate', 'cparun', 'cpawarn', 'cpaerr', 'cpatempwi', 'cpatempwo', 'cpatempo', 'cpatemph', \n",
    "                        'cpalp', 'cpalpa', 'cpahp', 'cpahpa', 'cpadp', 'cpacurrent', 'cpahours', 'cpapscale', \n",
    "                        'cpatscale', 'cpasn', 'cpamodel']    \n",
    "        date_col = []\n",
    "        ST_col = [[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]\n",
    "        is_first_log = True   \n",
    "        for date_log in list_date_log:\n",
    "            log_name = log_folder + date_log['str'] + '/Status_' + date_log['str'] + '.log'\n",
    "            \n",
    "            # Check whether file exists\n",
    "            if not os.path.isfile(log_name):\n",
    "                continue\n",
    "                \n",
    "            with open(log_name, 'r') as log_file:\n",
    "                log_lines = log_file.readlines()\n",
    "                \n",
    "                # Collect a column of data for each registered instant\n",
    "                for line in log_lines:\n",
    "                    line = line.replace('\\n', '').split(',')\n",
    "                    \n",
    "                    # Fill date column\n",
    "                    date = line[0].replace(' ','') + ' ' + line[1].replace(' ','')\n",
    "                    \n",
    "                    if not len(line) == 64:\n",
    "                        print('Status data missing/incomplete ::: ', date)\n",
    "                        continue\n",
    "                        \n",
    "                    date_col.append(pd.to_datetime(date, format = \"%d-%m-%y %H:%M:%S\"))\n",
    "                    \n",
    "                    # Fill each variable's data column.\n",
    "                    for i in range(len(column_names)):\n",
    "                        ST_col[i].append(float(line[3 + 2*i]))\n",
    "                    \n",
    "                    # Fill a line with NaN's if non-logged period is larger than 5 minutes\n",
    "                    if is_first_log:\n",
    "                        is_first_log = False\n",
    "                        continue\n",
    "\n",
    "                    if (pd.Timedelta(date_col[-1] - date_col[-2]).seconds)/60.0 > 5:\n",
    "                        non_logged = pd.date_range(start = date_col[-2], end = date_col[-1], periods = 3)[1]\n",
    "                        date_col.append(non_logged)\n",
    "                        for i in range(len(column_names)):\n",
    "                            ST_col[i].append(np.nan)\n",
    "                    \n",
    "        data_dict = {column_names[i]: ST_col[i] for i in range(len(column_names))}\n",
    "        index = pd.DatetimeIndex(date_col)\n",
    "        \n",
    "        return pd.DataFrame(data = data_dict, index = index).sort_index() \n",
    "    \n",
    "    \n",
    "    if data_type == 'Flowmeter':\n",
    "        \n",
    "        date_col = []\n",
    "        FL_col = []\n",
    "        \n",
    "        is_first_log = True   \n",
    "        for date_log in list_date_log:\n",
    "            log_name = log_folder + date_log['str'] + '/Flowmeter ' + date_log['str'] + '.log'\n",
    "            \n",
    "            # Check whether file exists\n",
    "            if not os.path.isfile(log_name):\n",
    "                continue\n",
    "                \n",
    "            with open(log_name, 'r') as log_file:\n",
    "                log_lines = log_file.readlines()\n",
    "                \n",
    "                # Collect a column of data for each registered instant\n",
    "                for line in log_lines:\n",
    "                    line = line.split(',')\n",
    "                    \n",
    "                    # Fill date column\n",
    "                    date = line[0].replace(' ','') + ' ' + line[1].replace(' ','')\n",
    "                    date_col.append(pd.to_datetime(date, format = \"%d-%m-%y %H:%M:%S\"))\n",
    "                    \n",
    "                    # Fill each flowmeter`s data column\n",
    "                    FL_col.append(float(line[2].replace('\\n', '')))\n",
    "                    \n",
    "                    # Fill a line with NaN's if non-logged period is larger than 5 minutes\n",
    "                    if is_first_log:\n",
    "                        is_first_log = False\n",
    "                        continue\n",
    "\n",
    "                    if (pd.Timedelta(date_col[-1] - date_col[-2]).seconds)/60.0 > 5:\n",
    "                        non_logged = pd.date_range(start = date_col[-2], end = date_col[-1], periods = 3)[1]\n",
    "                        date_col.append(non_logged)\n",
    "                        FL_col.append(np.nan)\n",
    "\n",
    "        return pd.DataFrame(data = {'Flowmeter': FL_col}, index = pd.DatetimeIndex(date_col)).sort_index() \n",
    "        \n",
    "    if data_type == 'Temperature':\n",
    "        \n",
    "        column_names = ['CH1', 'CH2', 'CH5', 'CH6']\n",
    "        date_col = [[], [], [], []]\n",
    "        CH_col = [[], [], [], []]\n",
    "        \n",
    "        for indx, variable in enumerate(column_names):\n",
    "            \n",
    "            is_first_log = True\n",
    "            \n",
    "            for date_log in list_date_log:\n",
    "                log_name = log_folder + '/log-data/192.168.109.188/' + date_log['str'] + '/%s T ' % variable + date_log['str'] + '.log'\n",
    "                \n",
    "                # Check whether file exists\n",
    "                if not os.path.isfile(log_name):\n",
    "                    continue\n",
    "                \n",
    "                with open(log_name, 'r') as log_file:\n",
    "                    \n",
    "                    log_lines = log_file.readlines()\n",
    "                    \n",
    "                    # Collect a line of data for each logged instant\n",
    "                    for line in log_lines:\n",
    "                        line = line.replace('\\n', '')\n",
    "                        line = line.split(',')\n",
    "                        \n",
    "                        # Fill date column\n",
    "                        date = pd.to_datetime(line[0].replace(' ','') + ' ' + line[1].replace(' ',''), format = \"%d-%m-%y %H:%M:%S\")\n",
    "                        date_col[indx].append(date)\n",
    "                        \n",
    "                        # Fill the channel's data column.\n",
    "                        CH_col[indx].append(float(line[2]))\n",
    "                        \n",
    "                        # Fill a line with NaN's if non-logged period is larger than 5 minutes\n",
    "                        if is_first_log:\n",
    "                            is_first_log = False\n",
    "                            continue\n",
    "                            \n",
    "                        if (pd.Timedelta(date_col[indx][-1] - date_col[indx][-2]).seconds)/60.0 > 5:\n",
    "                            non_logged = pd.date_range(start = date_col[indx][-2], end = date_col[indx][-1], periods = 3)[1]\n",
    "                            date_col[indx].append(non_logged)\n",
    "                            CH_col[indx].append(np.nan)\n",
    "                    \n",
    "        data_dict = {column_names[i]: CH_col[i] for i in range(len(column_names))}\n",
    "\n",
    "        return [pd.DataFrame(CH_col[i], \n",
    "                             index = pd.DatetimeIndex(date_col[i]), \n",
    "                             columns = [column_names[i]]).sort_index()\n",
    "                for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_timeseries(df, date_list, comparison_span = 3):\n",
    "    '''\n",
    "    This function slices a time-indexed timeseries df into multiple output timeseries \n",
    "    starting at given dates for a defined period of time.\n",
    "    \n",
    "    df: time-indexed single-column pandas dataframe.\n",
    "    date_list: list of datetime objects describing where each df slice should start.\n",
    "    comparison_span: time range of the data slice in hours.\n",
    "    '''\n",
    "    \n",
    "    out_df_number = len(date_list)\n",
    "    out_df = []\n",
    "    \n",
    "    for indx, start_date in enumerate(date_list):\n",
    "        end_date = start_date + datetime.timedelta(hours = comparison_span)\n",
    "        sliced_df = df[start_date:end_date]\n",
    "        reset_time_index = [x - sliced_df.index[0] for x in sliced_df.index]\n",
    "        out_df.append(pd.DataFrame(list(sliced_df), index = reset_time_index, columns = [sliced_df.name + '_%d' % indx]))\n",
    "        \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important\n",
    "2.00E-2 is the default reading value of P1 (maxigauge CH1) when it is turned off. First column of the maxigauge data is 0 when the sensor is turned off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "\n",
    "def plot_data(log_folder = 'log/'):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    stored_data = {}\n",
    "    '''\n",
    "    {\n",
    "        maxigauge: {\n",
    "            CH1:\n",
    "            ...\n",
    "            CH6:\n",
    "        }\n",
    "        Temperature: {\n",
    "            CH1:\n",
    "            CH2:\n",
    "            CH5:\n",
    "            CH6:\n",
    "        }\n",
    "        Flowmeter: \n",
    "    }\n",
    "    '''\n",
    "    \n",
    "    def df_slices(start_daytime, data_type, comparison_span):\n",
    "        \n",
    "        # Identify specific data to be plotted\n",
    "        channel = ''\n",
    "        split_data_type = data_type.split(' ')\n",
    "        if len(split_data_type) > 1:\n",
    "            channel = split_data_type[1]\n",
    "            data_type = split_data_type[0]\n",
    "        \n",
    "        # Convert start_daytime to date_list\n",
    "        date_list = []\n",
    "        for date in start_daytime.replace('\\n', '').split(','):\n",
    "            try:\n",
    "                date_list.append(pd.to_datetime(date.strip(), format = \"%d-%m-%y %H:%M:%S\"))\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        # If desired dataframe has not been ingested already, ingest, sort and store it\n",
    "        if not data_type in stored_data.keys():\n",
    "            df_all = log_parsing(log_folder, data_type)\n",
    "            \n",
    "            if data_type == 'Temperature':\n",
    "                stored_data[data_type] = {}\n",
    "                for i, ch in enumerate(['CH1', 'CH2', 'CH5', 'CH6']):\n",
    "                    stored_data[data_type][ch] = df_all[i].sort_index()[ch]\n",
    "                    \n",
    "            if data_type == 'maxigauge':\n",
    "                stored_data[data_type] = {}\n",
    "                for i, ch in enumerate(['CH1', 'CH2', 'CH3', 'CH4', 'CH5', 'CH6']):\n",
    "                    stored_data[data_type][ch] = df_all.sort_index()[ch]\n",
    "                    \n",
    "            if data_type == 'Flowmeter':\n",
    "                stored_data[data_type] = df_all.sort_index()\n",
    "        \n",
    "        # Retrieve only desired data for use\n",
    "        if channel:\n",
    "            df = stored_data[data_type][channel]\n",
    "        else:\n",
    "            df = stored_data[data_type]\n",
    "        \n",
    "        # Timeslice the data\n",
    "        sliced_df_list = slice_timeseries(df, date_list, comparison_span = comparison_span)\n",
    "        \n",
    "        # Prepare for plot\n",
    "        out_df_list = []\n",
    "        for i, df_slice in enumerate(sliced_df_list):\n",
    "            try:\n",
    "                out_df_list.append((df_slice.index.total_seconds(), \n",
    "                                    list(df_slice.iloc[:,0]), \n",
    "                                    date_list[i]))\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "        return out_df_list\n",
    "    \n",
    "    def format_timedelta_ticks(x, pos):                                                                                                                                                                                                                                                         \n",
    "        d = datetime.timedelta(seconds=x)  \n",
    "        return str(d).split(', ')[-1].strip() + '$_{+%s}$' % str(d.days)\n",
    "    \n",
    "    wd = widgets.IntText(value = 3,\n",
    "                         description = 'Timespan (h):',\n",
    "                          style = {'description_width': 'initial'},\n",
    "                         disabled = False)\n",
    "    \n",
    "    dt = widgets.Textarea(value = '01-01-00 00:00:00',\n",
    "                          description = 'Starting time',\n",
    "                          style = {'description_width': 'initial'},\n",
    "                          disabled = False)\n",
    "    \n",
    "    ty = widgets.Dropdown(options=['Flowmeter',\n",
    "                                   'maxigauge CH1', \n",
    "                                   'maxigauge CH2', \n",
    "                                   'maxigauge CH3', \n",
    "                                   'maxigauge CH4', \n",
    "                                   'maxigauge CH5', \n",
    "                                   'maxigauge CH6', \n",
    "                                   'Temperature CH1',\n",
    "                                   'Temperature CH2',\n",
    "                                   'Temperature CH5',\n",
    "                                   'Temperature CH6'],\n",
    "                          value = 'maxigauge CH5',\n",
    "                          description = 'Data type',\n",
    "                          style = {'description_width': 'initial'},\n",
    "                          #layout=Layout(width='50%', height='80px'),\n",
    "                          disabled = False)\n",
    "    \n",
    "    sc = widgets.Checkbox(value = False,\n",
    "                          description = 'Log scale',\n",
    "                          disabled = False,\n",
    "                          style = {'description_width': 'initial'},\n",
    "                          indent = False)\n",
    "    \n",
    "\n",
    "    \n",
    "    @widgets.interact(start_daytime = dt, comparison_span = wd, data_type = ty, log_scale = sc)\n",
    "    def update_plot(start_daytime, data_type, comparison_span, log_scale):\n",
    "        \n",
    "        ax.cla()\n",
    "        ax.grid(True)\n",
    "        ax.set_title(data_type)\n",
    "        \n",
    "        out_df_list = df_slices(start_daytime, data_type, comparison_span)\n",
    "        for out_df in out_df_list:\n",
    "            ax.plot(out_df[0], out_df[1], label = out_df[2].strftime(\"%d-%m-%y %H:%M:%S\"))\n",
    "            \n",
    "        ax.legend()\n",
    "        ax.xaxis.set_major_formatter(plt.FuncFormatter(format_timedelta_ticks))\n",
    "        ax.tick_params(axis='x', labelrotation = 25)\n",
    "        ax.set_yscale('log' if log_scale else 'linear')\n",
    "        ax.autoscale_view()\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        return\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a936622d5db541ec80b9d2f713f9d2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c88d7ba028145ed9e61eac569874d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Textarea(value='01-01-00 00:00:00', description='Starting time', style=DescriptionStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data('log/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
